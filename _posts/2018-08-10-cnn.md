---
title: 'A Minimal Deep Learning Library Implementation In Python'
date: 2018-08-10
permalink: /posts/2018/08/cnn/
name: cnn
tags:
  - Deep Learning
  - Library
  - PyTorch
---

<p style="display:none">
$
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\svert}{~|~}
\newcommand{\f}{\mathbf{f}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\ls}{\mathbf{l}}
\newcommand{\bL}{\mathbf{L}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\p}{\mathbf{p}}
\newcommand{\bepsilon}{\text{$\epsilon$}}
\newcommand{\bgamma}{\text{$\gamma$}}
\newcommand{\K}{\mathbf{K}}
\newcommand{\diag}{\text{diag}}
\newcommand{\argmin}{\text{argmin}}
$
</p>
<p>This tutorial introduces the main blocks needed to build a <a href="https://github.com/emilemathieu/blog_cnn">deep learning library</a> in a few lines of Python. You'll learn how to build your (very) light-weight version of <a href="https://pytorch.org">PyTorch</a>!
  </p>

  <br>
<h2>Convolutional Neural Networks</h2>
<p>
We'll assume familiarity with Convolutional Neural Network (CNN) models. If not I strongly recommend you to first read <a href="http://cs231n.github.io/convolutional-networks/">Stanford's CS231n course on CNNs</a>.
    
    <div style="margin-top: -20px; margin-bottom: 20px; max-width: 80%; display: block; margin-left: auto; margin-right: auto" class="image captioned row 100% special">
        <div class="12u 12u$(medium)"><span class="image captioned fit"><img src="{{site.baseurl}}/images/blog/cnn/cnn.png" alt=""></span></div>
        <div class="12u"><h5>Architecture of a CNN. — Source: <a href="https://www.mathworks.com/videos/introduction-to-deep-learning-what-are-convolutional-neural-networks--1489512765771.html"> https://www.mathworks.com.</a></h5></div>
    </div>
</p>


<h2>The <i>module</i> paradigm</h2>
<p>The module inheritance paradigm underlies the widely used <a href="https://pytorch.org">PyTorch</a>, <a href="https://github.com/deepmind/sonnet">Sonnet</a> and <a href="https://gluon.mxnet.io">Gluon</a> deep learning libraries.
The idea is to build neural networks in a hierarchical and factored way by leveraging the modularity of so-called <i>modules</i>.
<br>
From a mathematical perspective a <i>module</i> is a differentiable function. It can  have parameters to be learned and then needs to also be differentiable with respect to these parameters.
From a programmatic perspective, it is a class implementing a <b>forward</b> method $\mathbf{y} = f(\mathbf{x})$ (input $\mathbf{x}$, output $\mathbf{y}$) and a <b>backward</b> method $g(\mathbf{x}, \nabla\mathbf{y}) = \nabla_{\mathbf{x}} \ f(\mathbf{x}) \times \nabla \mathbf{y}$. If it has parameters $\theta$ that we wish to learn, it additionally needs these parameters as <b>attributes</b>, a <b>step</b> method that performs an optimisation step on $\theta$ when called, and the <b>backward</b> method to also compute and save $\nabla_{\theta} f_\theta(\mathbf{x}) \times \nabla\mathbf{y}$ before returning $g(\mathbf{x}, \nabla\mathbf{y})$.
Below is the abstract <b>Module</b> class that will be inherited.
</p>

<script src="https://gist.github.com/emilemathieu/f640d0a7d368196f39f25202b1f95d6d.js"></script>

<p>This design enables to build small blocks of layers than can then be assembled to build bigger blocks and so on. Factoring code in this manner reduces the number of lines needed to defined a model and eases experimentation of new architectures.
</p>

<br>
<h3>Layers</h3>
<p>
Layers are the basic functions composing neural networks.
They will be implemented by inheriting the abstract <b>Module</b> class.
For instance, a <i>linear layer</i> is parametrised by a weight $\mathbf{A}$ and a bias $\mathbf{b}$, and defined by the <b>forward</b> function $f(\mathbf{x}) = \mathbf{A} \mathbf{x} + \mathbf{b}$, the <b>backward</b> function $g(\mathbf{x}, \nabla\mathbf{y}) = \nabla_{\mathbf{x}} \ f(\mathbf{x}) \times \nabla \mathbf{y} = \mathbf{A} \nabla \mathbf{y}$ and its weight and bias partial derivatives 
$\nabla_{\mathbf{b}} \ f_{\mathbf{A},\mathbf{b}}(\mathbf{x}) \times \nabla\mathbf{y} = 1^T \nabla\mathbf{y}$
and $\nabla_{\mathbf{A}} \ f_{\mathbf{A},\mathbf{b}}(\mathbf{x}) \times \nabla\mathbf{y} = \nabla\mathbf{y}^T \mathbf{x}$.
</p>
<script src="https://gist.github.com/emilemathieu/87938f3e9e9acd00372dc7224a832574.js"></script>

<br>
<h3>Training</h3>
<p>
Neural networks can be trained via <i>back-propagation</i> which is the application of the <a href="https://en.wikipedia.org/wiki/Chain_rule"><i>chain rule</i></a> to the loss's gradient up to the neural net's layers parameters.
For easiness, we restrict ourselves to neural nets that can be represented as a composition of functions $f(\mathbf{x}) = f_n \circ \dots \circ f_1(\mathbf{x})$. It enables to represent explicitly the dependency between layers via a list. Some networks like <a href="https://arxiv.org/abs/1512.03385">ResNets</a> are excluded by this assumption since they need the network to be represented as a DAG, but it's all right for our purpose.
<br><br>
Such composition of functions are implemented via a <b>Sequential</b> class constructed with a list of sub-modules. Then its <b>forward</b> method calls each sub-module's <b>forward</b> method, its <b>backward</b> calls in <i>reverse</i> order each sub-module's <b>backward</b> method, and its <b>step</b> method calls each trainable (having parameters to learn) sub-module's <b>step</b> method.
</p>
<script src="https://gist.github.com/emilemathieu/97dc38a4c0f8a4d66393c19d73df24b2.js"></script>

<p>
    The network is trained so as to minimise a user-defined loss $L(\mathbf{X}, \mathbf{y}^{\text{true}}, \theta) = \sum_{i} l(f_{\theta}(\mathbf{x}_i), y{\text{true}}_i)$. The <i>cross entropy</i> is for instance widely used for classification tasks.
</p>
<script src="https://gist.github.com/emilemathieu/1873f05800df51927a1d7e4dbb330b32.js"></script>

<p>
Hence, each trainable layer gets its parameters partial derivative automatically computed by backpropagating the loss's gradient through the sequence of layers:
\begin{equation}
\forall i=n,\dots,1  \quad  \nabla_{\theta_i} L(\mathbf{X}, \mathbf{y}, \theta) = \nabla_{\theta_i} f_i(\mathbf{X}) \times \left( g_{i+1} \circ \dots \circ g_n \left(\nabla_{X} L \left(\mathbf{X}, \mathbf{y}, \theta\right) \right) \right)
\end{equation}
</p>

<h2>Optimisation</h2>
<p>
The most straightforward optimisation scheme for neural networks is (momentum) stochastic gradient descent which applies the following update on the parameters $\theta$

\begin{equation}
v_t = \gamma v_{t-1} + \eta \nabla_\theta J( \theta) \\  
\theta = \theta - v_t
\end{equation}
with $\eta$ being the learning rate and $\gamma$ the momentum coefficient.
<b>SGD</b> inherits from <b>Optmizer</b> as implemented below.
</p>
<script src="https://gist.github.com/emilemathieu/46fb760060ea4b28bad28f3035c26a21.js"></script>
<p>
I also implemented <a href="https://arxiv.org/abs/1412.6980">Adam</a> and <a href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">RMSProp</a> in <a href="https://github.com/emilemathieu/blog_cnn/blob/master/mllib/optim.py">optim.py</a>. For a nice review of gradient-based optimisation schemes used for deep neural nets training, check <a href="http://ruder.io/optimizing-gradient-descent/">S. Ruder's blog post</a>.

<br><br>
An optimisation step can then be performed by calling
<script src="https://gist.github.com/emilemathieu/8525404649bb6c786243225d4f66c77d.js"></script>
</p>

<br>
<h2>Demonstration on MNIST</h2>
<p>
Let's demonstrate our library on the <a href="http://yann.lecun.com/exdb/mnist/">MNIST classification task dataset</a>.
We define our CNN by inheriting the <b>Module</b> class, thus implementing the <b>forward</b>, <b>backward</b> and <b>step</b> methods as shown below.

</p>
<script src="https://gist.github.com/emilemathieu/acb3dc9a0125bd9e30b69ff857cc3eec.js"></script>
<p>
    Clone the <a href="https://github.com/emilemathieu/blog_cnn">associated Github repository</a> and then run the <a href="https://github.com/emilemathieu/blog_cnn/blob/master/example.py">example script</a> so as to load the MNIST dataset, instantiate the CNN model, train it by backpropagation and eventually predict a digit's label as plotted below.

<div class="image captioned row 100% special">
        <div style="float: left; width: 50%; padding: 5px"><img style="width:100%" src="{{site.baseurl}}/images/blog/cnn/digit.png" alt=""></div>
        <div style="float: left; width: 50%; padding: 5px"><img style="width:100%" src="{{site.baseurl}}/images/blog/cnn/pred.png" alt=""></div>
        <div class="12u"><h5>(Left) Example of mnist digit. (Right) Label prediction of our trained CNN.</h5></div>
    </div>
</p>

<h3>What's missing towards a complete library ?</h3>
<p>
We've built in this tutorial the minimum blocks towards a working deep learning library, yet some parts are still missing to have a complete library.
First, an <b>automatic-differentation</b> (autodiff) library avoid having to explicitly manipulate the gradients. Indeed, while being (imperatively) defined, the neural net is then implicitly represented as a DAG, allowing gradients to be automatically computed in a backward manner via the <i>chain rule</i>.
Moreover, a <b>data-loader</b> wrapper is really useful so as to ease downloading, loading and preprocessing datasets. More layers and optimisation schemes should also be added.
<b>GPU support</b> is also really important for large scale training.
</p>

<h3>Acknowledgments</h3>
<p>I’m grateful to Thomas Pesneau and Yuan Zhou for their comments.</p>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

var disqus_config = function () {
this.page.url = 'http://emilemathieu.fr';  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = 'cnn'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://http-emilemathieu-fr.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>