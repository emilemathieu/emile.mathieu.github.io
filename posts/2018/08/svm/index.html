

<!doctype html>
<html lang="en" class="no-js">
  <head>
    <script src="https://emilemathieu.fr/assets/js/theme-checker.js"></script>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>An Efficient Soft-Margin Kernel SVM Implementation In Python - Emile Mathieu</title>







<meta property="og:locale" content="en-UK">
<meta property="og:site_name" content="Emile Mathieu">
<meta property="og:title" content="An Efficient Soft-Margin Kernel SVM Implementation In Python">


  <link rel="canonical" href="https://emilemathieu.fr/posts/2018/08/svm/">
  <meta property="og:url" content="https://emilemathieu.fr/posts/2018/08/svm/">



  <meta property="og:description" content="$\newcommand{\R}{\mathbb{R}}\newcommand{\N}{\mathcal{N}}\newcommand{\svert}{~|~}\newcommand{\f}{\mathbf{f}}\newcommand{\x}{\mathbf{x}}\newcommand{\y}{\mathbf{y}}\newcommand{\z}{\mathbf{z}}\newcommand{\w}{\mathbf{w}}\newcommand{\W}{\mathbf{W}}\newcommand{\ba}{\mathbf{a}}\newcommand{\m}{\mathbf{m}}\newcommand{\ls}{\mathbf{l}}\newcommand{\bL}{\mathbf{L}}\newcommand{\X}{\mathbf{X}}\newcommand{\Y}{\mathbf{Y}}\newcommand{\p}{\mathbf{p}}\newcommand{\bepsilon}{\text{$\epsilon$}}\newcommand{\bgamma}{\text{$\gamma$}}\newcommand{\K}{\mathbf{K}}\newcommand{\diag}{\text{diag}}\newcommand{\argmin}{\text{argmin}}$This short tutorial aims at introducing support vector machine (SVM) methods from its mathematical formulation along with an efficient implementation in a few lines of Python! Do play with the full code hosted on my github page. I strongly recommend reading Support Vector Machine Solvers (from L. Bottou &amp; C-J. Lin) for an in-depth cover of the topic, along with the LIBSVM library. The present post naturally follows this introduction on SVMs.">





  

  





  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2018-08-08T00:00:00+01:00">








  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Emile Mathieu",
      "url" : "https://emilemathieu.fr",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="https://emilemathieu.fr/feed.xml" type="application/atom+xml" rel="alternate" title="Emile Mathieu Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<!-- <link rel="stylesheet" href="https://emilemathieu.fr/assets/css/main.css"> -->
<link rel="stylesheet" href="/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="https://emilemathieu.fr/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="https://emilemathieu.fr/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="https://emilemathieu.fr/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="https://emilemathieu.fr/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="https://emilemathieu.fr/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="https://emilemathieu.fr/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="https://emilemathieu.fr/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="https://emilemathieu.fr/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="https://emilemathieu.fr/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="https://emilemathieu.fr/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="https://emilemathieu.fr/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="https://emilemathieu.fr/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="https://emilemathieu.fr/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="manifest" href="https://emilemathieu.fr/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="https://emilemathieu.fr/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="https://emilemathieu.fr/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="https://emilemathieu.fr/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="https://emilemathieu.fr/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="https://emilemathieu.fr/">Emile Mathieu</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://emilemathieu.fr/papers/">Papers</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://emilemathieu.fr/talks/">Talks</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://emilemathieu.fr/teaching/">Teaching</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://emilemathieu.fr/blog/">Blog</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://emilemathieu.fr/cv/">CV</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://emilemathieu.fr/contact/">Contact</a></li>
          
          <!-- <button id="theme-toggle" onclick="modeSwitcher()"></button> -->
          <li class="masthead__menu-item">
          <div class="container">
            <input type="checkbox" id="theme-toggle" onclick="modeSwitcher()">
            <label for="theme-toggle"></label>
          </div>
          </li>
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





<div id="main" role="main">
  


  <div class="sidebar sticky">
  



<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="https://emilemathieu.fr/images/profile2.jpg" class="author__avatar" alt="Emile Mathieu">
    
  </div>

  <div class="author__content">
    <h3 class="author__name">Emile Mathieu</h3>
    <p class="author__bio">Machine Learning Research Scientist</p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> London, UK</li>
      
      
      
      
      
       
      
        <li><a href="https://twitter.com/MathieuEmile"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
      
      
      
      
      
      
      
      
      
        <li><a href="https://github.com/emilemathieu"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com/citations?user=g9BjTqgAAAAJ"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
      
      
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="An Efficient Soft-Margin Kernel SVM Implementation In Python">
    <meta itemprop="description" content="$\newcommand{\R}{\mathbb{R}}\newcommand{\N}{\mathcal{N}}\newcommand{\svert}{~|~}\newcommand{\f}{\mathbf{f}}\newcommand{\x}{\mathbf{x}}\newcommand{\y}{\mathbf{y}}\newcommand{\z}{\mathbf{z}}\newcommand{\w}{\mathbf{w}}\newcommand{\W}{\mathbf{W}}\newcommand{\ba}{\mathbf{a}}\newcommand{\m}{\mathbf{m}}\newcommand{\ls}{\mathbf{l}}\newcommand{\bL}{\mathbf{L}}\newcommand{\X}{\mathbf{X}}\newcommand{\Y}{\mathbf{Y}}\newcommand{\p}{\mathbf{p}}\newcommand{\bepsilon}{\text{$\epsilon$}}\newcommand{\bgamma}{\text{$\gamma$}}\newcommand{\K}{\mathbf{K}}\newcommand{\diag}{\text{diag}}\newcommand{\argmin}{\text{argmin}}$This short tutorial aims at introducing support vector machine (SVM) methods from its mathematical formulation along with an efficient implementation in a few lines of Python! Do play with the full code hosted on my github page. I strongly recommend reading Support Vector Machine Solvers (from L. Bottou &amp; C-J. Lin) for an in-depth cover of the topic, along with the LIBSVM library. The present post naturally follows this introduction on SVMs.">
    <meta itemprop="datePublished" content="August 08, 2018">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">An Efficient Soft-Margin Kernel SVM Implementation In Python
</h1>
          
            <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  9 minute read
	
</p>
          
        
        
        
          <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2018-08-08T00:00:00+01:00">August 08, 2018</time></p>
        
        
             
        
    
        </header>
      

      <section class="page__content" itemprop="text">
        <p style="display:none">
$
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\svert}{~|~}
\newcommand{\f}{\mathbf{f}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\ls}{\mathbf{l}}
\newcommand{\bL}{\mathbf{L}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\p}{\mathbf{p}}
\newcommand{\bepsilon}{\text{$\epsilon$}}
\newcommand{\bgamma}{\text{$\gamma$}}
\newcommand{\K}{\mathbf{K}}
\newcommand{\diag}{\text{diag}}
\newcommand{\argmin}{\text{argmin}}
$
</p>
<p>This short tutorial aims at introducing support vector machine (SVM) methods from its mathematical formulation along with an efficient implementation in a few lines of Python! Do play with the full code hosted on <a href="https://github.com/emilemathieu/ImageClassificationChallenge/tree/master/code/mllib/svm" target="_blank">my github page</a>. I strongly recommend reading <a href="http://leon.bottou.org/publications/pdf/lin-2006.pdf" target="_blank">Support Vector Machine Solvers</a> (from L. Bottou &amp; C-J. Lin) for an in-depth cover of the topic, along with the <a href="http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf" target="_blank">LIBSVM</a> library. The present post naturally follows this <a href="http://tullo.ch/articles/svm-py/" target="_blank">introduction on SVMs</a>.</p>

<h2>Support Vector Machines - The model</h2>

<p>
<div><span class="image captioned right" style="max-width: 300px;float: right"><img src="/images/blog/svm/optimal-hyperplane.png" alt="" /><h5>Figure 1: <i>An optimal hyperplane.</i></h5></span></div>
<a href="https://en.wikipedia.org/wiki/Support_vector_machine" target="_blank">Support vector machines</a> (SVMs) are supervised learning models for classification (or regression) defined by <i>supporting hyperplanes</i>.
SVM is one of the most widely used algorithms since it relies on strong theoretical foundations and has good performance in practice.
<br /><br />
  As illustrated on Figure 1, SVMs represent examples as points in space, mapped so that the examples of the different categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and assigned a category based on which side of the gap they fall.
<!-- This hyperplane is optimal in the sense that if the data is separable, it is the one with the maximum margin as shown on the Figure 1. -->
<br /><br />
Let's consider a dataset
<!-- \begin{equation} -->
$D = { (\mathbf{x}_{i}, y_{i}), \mathbf{x} \in \mathbb{R}^d, y \in \{ -1, 1 \}}$
<!-- \end{equation} -->
, and $\phi$ a feature mapping - a possibly non-linear function used to get features from the datapoints $\{x_i\}$.
<br />
The main idea is to find an hyperplane $\w^*$ separating our dataset and maximising the <i>margin</i> of this hyperplane:
<br /><br />
\begin{equation} \label{eq:hard_objective}
\min _{\w, b} \mathcal{P}(\w, b) = \cfrac{1}{2} \w^2
  \end{equation}
  \begin{equation} \label{eq:hard_conditions}
\text{subject to} \  \forall i \ \ y_i(\w^T \phi(\x_i)+b) \ge 1
\end{equation}
The conditions (\ref{eq:hard_conditions}) enforce that datapoints (after being mapped through $\phi$) from the first (respectively second) category lie below (respectively above) the hyperplane $\w^*$, while the objective (\ref{eq:hard_objective}) maximises the <i>margin</i> (the distance between the hyperplane $\w^*$ and the closest example).
</p>
<h3> Soft margin</h3>
<p>
We implicitly made the assumption that the dataset $D$ was <i>separable</i>: that there exists an hyperplane $\w^* \in \mathbb{R}^d$ such that all red points  (i.e. $y=-1)$ lie on one side of the hyperplane (i.e. $\w^T \phi(\x_i)+b \le 0$) and blue points (y=$+1)$ lie on the other side of the hyperplane (i.e. $\w^T \phi(\x_i)+b \ge 0$).
This formulation is called <i>hard margin</i>, since the margin cannot let some datapoints <i>go through</i> (all datapoints are well classified).
<br />This assumption can be relaxed by introducing positive slack variables $\mathbf{\xi}=(\xi_1, \dots, \xi_n)$
allowing some examples to violate the margin constraints (\ref{eq:hard_conditions}).
$\xi_i$ are non-zero only if $\x_i$ sits on the wrong side of the hyperplane, and is equal to the distance between $\x_i$ and the hyperplane $\w$.
Then an hyperparameter $C$ controls the compromise between large margins and small margin violations.
<!-- This is formalised as the following constrained optimisation problem: -->
</p>

<!-- <h3>Kernel Trick</h3>
<p>Although SVMs are linear models, they can perform non-linear classification/regression using the so-called <i>kernel trick</i>, by implicitly mapping their inputs into high-dimensional feature spaces.</p> -->

<!-- <h3> Formulation </h3> -->

<p>
\begin{equation} \label{eq:hard_primal}
\min _{\w, b, \mathbf{\xi}} \mathcal{P}(\w, b, \mathbf{\xi}) = \cfrac{1}{2} \w^2 + C \sum_{i=1}^n \xi_i \\
\text{subject to} \begin{cases} \forall i \quad y_i(\w^T \phi(\x_i)+b) \ge 1 - \xi_i \\ 
\forall i \quad \xi_i \ge 0 \end{cases}
\end{equation}



</p>

<!-- <code data-gist-id="https://gist.github.com/emilemathieu/c8873e7a3f1aca7d123bea7bc9305627.js"></code> -->

<h2>How to fit the model ?</h2>
<p>Once one have such a mathematical representation of the model through an optimisation problem, the next natural question arising is <i> how should we solve this problem (once we know it is well-posed) ?</i>
The SVM solution is the optimum of a well defined convex optimisation problem (\ref{eq:hard_primal}).
Since the optimum does not depend on the manner it has been calculated, the choice of a particular optimisation algorithm can be made on the sole basis of its computational requirements.
<!-- The Python's method <i>_compute_weights</i> can therefore be implemented in several ways, and we'll present an efficient implementation. -->
</p>

<h3>Dual formulation</h3>

<p>
Directly solving (\ref{eq:hard_primal}) is difficult because the constraints are quite complex.
A classic move is then to simplify this problem via <i>Lagrangian duality</i> (see <a href="http://leon.bottou.org/publications/pdf/lin-2006.pdf">L. Bottou et al</a> for more details), yielding the <i>dual</i> optimisation problem:

\begin{equation} \label{eq:soft_dual}
\max _{\alpha} \mathcal{D}(\alpha) = \sum_{i=1}^n \alpha_i - \cfrac{1}{2} \sum_{i,j=1}^n y_i \alpha_i y_j \alpha_j \mathbf{K}(\x_i, \x_j) \\
\end{equation}  
\begin{equation}  \label{eq:soft_dual_cons}
\text{subject to} \begin{cases} \forall i \quad 0 \le \alpha_i \le C \\
\sum_i y_i\alpha_i = 0 \end{cases}
\end{equation} 

with $\{\alpha_i\}_{i=1,\dots,n}$ being the dual coefficients to solve and $\mathbf{K}$ being the kernel associated with $\phi$: $\forall i,j \ \ \mathbf{K}(\x_i, \x_j)=\left&lt; \phi(\x_i) , \phi(\x_j)\right&gt;$. That problem is much easier to solve since the constraints are much simpler.
Then, the direction $\w^*$ of the optimal hyperplane is recovered from a solution $\alpha^*$ of the dual optimisation problem (\ref{eq:soft_dual}-\ref{eq:soft_dual_cons}) (by forming the Lagragian and taking its minimum w.r.t. $\w$ - which is a strongly convex function):
\begin{equation}
\w^* = \sum_{i} \alpha^*_i y_i \phi(\x_i)
\end{equation}

The optimal hyperplane is therefore a weighted combination over the datapoints with non-zero dual coefficient $\alpha^*_i$. Those datapoints are therefore called <i>support vectors</i>, hence <i>«support vector machines»</i>. This property is quite elegant and really useful since in practice only a few $\alpha^*_i$ are non-zeros. Hence, a new datapoint prediction only requires to evaluate:

\begin{equation}
\text{sign}\left(\w^{*T} \phi(\x)+b\right) = \text{sign}\left(\sum_{i} \alpha^*_i y_i \phi(\x_i)^T\phi(\mathbf{x}) +b\right)
= \text{sign}\left(\sum_{i} \alpha^*_i y_i \mathbf{K}(\mathbf{x}_i, \mathbf{x}) +b \right)
\end{equation}
</p>

<h3>Quadratic Problem solver</h3>
<p>
The SVM optimisation problem (\ref{eq:soft_dual}) is a Quadratic Problem (QP), a well studied class of optimisation problems for which  good libraries has been developed for.
This is the approach taken in this <a href="http://tullo.ch/articles/svm-py/">intro on SVM</a>, relying on the Python's quadratic program solver <a href="http://cvxopt.org">cvxopt</a>.
<br />
Yet this approach can be inefficient since such packages were often designed to take advantage of sparsity in the quadratic part of the objective function. Unfortunately, the SVM kernel matrix $\mathbf{K}$ is rarely sparse but sparsity occurs in the <i>solution</i> of the SVM problem.
Moreover, the specification of a SVM problem rarely fits in memory and generic optimisation packages sometimes make extra work to locate the optimum with high accuracy which is often useless.
Let's then described an algorithm tailored to efficiently solve that optimisation problem.

</p>

<h3>The Sequential Minimal Optimisation (SMO) algorithm</h3>
<p>
One way to avoid the inconveniences above-mentioned is to rely on the decomposition method.
The idea is to decompose the optimisation problem in a sequence of subproblems where only a subset of coefficients $\alpha_i$, $i \in \mathcal{B}$ needs to be optimised, while leaving the remaining coefficients $\alpha_j$, $j \notin \mathcal{B}$ unchanged:

\begin{equation} \label{eq:smo}
\max _{\alpha'} \mathcal{D}(\alpha') = \sum_{i=1}^n \alpha'_i - \cfrac{1}{2} \sum_{i,j=1}^n y_i \alpha'_i y_j \alpha'_j \mathbf{K}(\x_i, \x_j) \\
\text{subject to} \begin{cases} \forall i \notin \mathcal{B}  \quad \alpha'_i=\alpha_i  \\
\forall i \in \mathcal{B} \quad 0 \le \alpha'_i \le C \\
\sum_i y_i\alpha'_i = 0 \end{cases}
\end{equation}

<!-- or equivalently

\begin{equation}
\max _{\alpha'} \mathcal{D}(\alpha') = \sum_{i=1}^n \alpha'_i - \cfrac{1}{2} \sum_{i,j=1}^n y_i \alpha'_i y_j \alpha'_j \mathbf{K}(\x_i, \x_j) \\
\text{subject to} \begin{cases} \forall i \notin \mathcal{B}  \quad \alpha'_i=\alpha_i  \\
\forall i \in \mathcal{B} \quad 0 \le \alpha'_i \le C \\
\sum_i y_i\alpha'_i = 0 \end{cases}
\end{equation} -->

One need to decide how to choose the working set $\mathcal{B}$ for each subproblem. The simplest is to always use the smallest possible working set, that is, two elements (such as the <i>maximum violating pair scheme</i>, which is discussed in Section 7.2 in <a href="http://leon.bottou.org/publications/pdf/lin-2006.pdf" target="_blank">Support Vector Machine Solvers</a> ). The equality constraint $\sum_i y_i \alpha'_i = 0$ then makes this a <i>one dimensional</i> optimisation problem.

<br /><br />
<div><span class="image captioned right" style="max-width: 450px;float: right"><img src="/images/blog/svm/directon_search.png" alt="" /><h5>Figure 2: <i>Direction search - from L. Bottou &amp; C-J. Lin.</i></h5></span></div>
The subproblem optimisation can then be achieved by performing successive <i>direction searches</i> along well chosen successive directions.
Such a method seeks to maximizes an optimisation problem restricted to the half line ${\mathbf{\alpha} + \lambda \mathbf{u}, \lambda \in \Lambda}$, with $\mathbf{u} = (u_1,\dots,u_n)$ a <i>feasible direction</i> (i.e. can slightly move the point $\mathbf{\alpha}$ along direction $\mathbf{u}$ without violating the constraints).

<br />
The equality constraint (\ref{eq:smo}) restricts $\mathbf{u}$ to the linear subspace $\sum_i y_i u_i = 0$.
Each subproblem is therefore solved by performing a search along a direction $\mathbf{u}$ containing only two non zero coefficients: ${u}_i = y_i$ and ${u}_j = −y_j$.

<br /><br />
The set $\Lambda$ of all coefficients $\lambda \ge 0$ is defined such that the point $\mathbf{\alpha} + \lambda \mathbf{u}$ satisfies the constraints. Since the feasible polytope is convex and bounded $\Lambda = [0, \lambda^{\max}]$.
Direction search is expressed by the simple optimisation problem
\begin{equation}
\lambda^* =  \arg\max_{\lambda \in \Lambda}{\mathcal{D}(\mathbf{\alpha }+ \lambda \mathbf{u})}
\end{equation}

Since the dual objective function is quadratic, $\mathcal{D}(\mathbf{\alpha }+ \lambda \mathbf{u})$ is shaped like a parabola. The location of its maximum $\lambda^+$ is easily computed using Newton’s formula:

\begin{equation}
\lambda^+ = \cfrac{ \partial \mathcal{D}(\mathbf{\alpha }+ \lambda \mathbf{u}) / \partial \lambda \ |_{\lambda=0} } {\partial^2 \mathcal{D}(\mathbf{\alpha }+ \lambda \mathbf{u}) / \partial \lambda^2 \ |_{\lambda=0}} = \cfrac{\mathbf{g}^T \mathbf{u}}{\mathbf{u}^T \mathbf{H} \mathbf{u}}
\end{equation}
</p>
<p>
where vector $\mathbf{g}$ and matrix $\mathbf{H}$ are the gradient and the Hessian of the dual objective function $\mathcal{D}(\mathbf{\alpha})$:

\begin{equation}
g_i  = 1 - y_i \sum_j{y_j \alpha_j K_{ij}} \quad \text{and} \quad H_{ij} = y_i y_j K_{ij}
\end{equation}

Hence $\lambda^* = \max \left(0, \min \left(\lambda^{\max}, \lambda^+ \right)\right) = \max \left(0, \min \left(\lambda^{\max}, \cfrac{\mathbf{g}^T \mathbf{u}}{\mathbf{u}^T \mathbf{H} \mathbf{u}} \right)\right)$.

<!--  The <i>Modified Gradient Projection</i> method further restrict the choice of the successive search directions $\mathbf{u}$ to conjugate successive search directions  -->

<!-- <br><br>
I won't go into details on the way to select the working sets is the maximum violating pair scheme, which is discussed in Section 7.2 in <a href="http://leon.bottou.org/publications/pdf/lin-2006.pdf" target="_blank" >Support Vector Machine Solvers</a> (from L. Bottou & C. Lin). -->
</p>

<h3>Implementation</h3>

<p>From a Python’s class point of view, an SVM model can be represented via the following attributes and methods:
<script src="https://gist.github.com/emilemathieu/c8873e7a3f1aca7d123bea7bc9305627.js"></script></p>

<p>Then the <b>_compute_weights</b> method is implemented using the SMO algorithm described above:
<script src="https://gist.github.com/emilemathieu/10f0b4a596c5ad571d8356f426b128a9.js"></script></p>

<!-- kernel trick; different kernels can be used such as the classic ones below -->
<!-- <script src="https://gist.github.com/emilemathieu/026df8545b880e7814ddc081a55a0e70.js"></script> -->

<!-- <h3>Demonstration</h3> -->

<p><br /><br /></p>
<h2>Demonstration</h2>
<p>
We demonstrate this algorithm on a synthetic dataset drawn from a two dimensional standard normal distribution.
Running the <a href="https://github.com/emilemathieu/blog_svm/blob/master/code/example.py">example script</a> will generate the synthetic dataset, then train a kernel SVM via the SMO algorithm and eventually plot the predicted categories.
</p>

<div class="image captioned row 100% special">
        <div class="6u 6u$(medium)"><span class="image captioned fit"><img src="/images/blog/svm/plot1.pdf" alt="" /></span></div>
        <div class="6u 6u$(medium)"><span class="image captioned fit"><img src="/images/blog/svm/plot2.pdf" alt="" /></span></div>
        <div class="12u"><h5>Optimal hyperplane with predicted labels for radial basis (left) and linear (right) kernel SVMs.</h5></div>
    </div>

<p>
The material and code is available on <a href="https://github.com/emilemathieu/ImageClassificationChallenge/tree/master/code/mllib/svm" target="_blank">my github page</a>. I hope you enjoyed that tutorial !
</p>

<h3>Acknowledgments</h3>
<p>I’m grateful to Thomas Pesneau for his comments.</p>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

var disqus_config = function () {
this.page.url = 'http://emilemathieu.fr';  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = 'svm'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://http-emilemathieu-fr.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>


        
      </section>

      <footer class="page__meta">
        
        


  




  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="https://emilemathieu.fr/tags/#quadratic-problem" class="page__taxonomy-item" rel="tag">Quadratic Problem</a><span class="sep">, </span>
    
      
      
      <a href="https://emilemathieu.fr/tags/#sequential-minimal-optimisation" class="page__taxonomy-item" rel="tag">Sequential Minimal Optimisation</a><span class="sep">, </span>
    
      
      
      <a href="https://emilemathieu.fr/tags/#support-vector-machine" class="page__taxonomy-item" rel="tag">Support Vector Machine</a>
    
    </span>
  </p>




      </footer>

      

<section class="page__share">
  

  <a href="https://twitter.com/intent/tweet?text=https://emilemathieu.fr/posts/2018/08/svm/" class="btn btn--twitter" title="Share on Twitter"><i class="fab fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https://emilemathieu.fr/posts/2018/08/svm/" class="btn btn--facebook" title="Share on Facebook"><i class="fab fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://emilemathieu.fr/posts/2018/08/svm/" class="btn btn--linkedin" title="Share on LinkedIn"><i class="fab fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>

      


  <nav class="pagination">
    
      <a href="https://emilemathieu.fr/posts/2018/08/pubs/" class="pagination--pager" title="10 Best Beer Gardens In Oxford
">Previous</a>
    
    
      <a href="https://emilemathieu.fr/posts/2018/08/cnn/" class="pagination--pager" title="A Minimal Deep Learning Library Implementation In Python
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      
      <div class="grid__wrapper">
        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://emilemathieu.fr/posts/2024/01/FM/" rel="permalink">An Introduction to Flow Matching
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  less than 1 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2024-01-20T00:00:00+00:00">January 20, 2024</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>
Flow matching (FM) is a recent generative modelling paradigm which has rapidly been gaining popularity in the deep probabilistic ML community. Flow matching combines aspects from Continuous Normalising Flows (CNFs) and Diffusion Models (DMs), alleviating key issues both methods have. In this blogpost we’ll cover the main ideas and unique properties of FM models starting from the basics.
</p>

</p>
    
    
    

  </article>
</div>

        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://emilemathieu.fr/posts/2018/08/cnn/" rel="permalink">A Minimal Deep Learning Library Implementation In Python
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  5 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2018-08-10T00:00:00+01:00">August 10, 2018</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p style="display:none">
$
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\svert}{~|~}
\newcommand{\f}{\mathbf{f}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\ls}{\mathbf{l}}
\newcommand{\bL}{\mathbf{L}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\p}{\mathbf{p}}
\newcommand{\bepsilon}{\text{$\epsilon$}}
\newcommand{\bgamma}{\text{$\gamma$}}
\newcommand{\K}{\mathbf{K}}
\newcommand{\diag}{\text{diag}}
\newcommand{\argmin}{\text{argmin}}
$
</p>
<p>This tutorial introduces the main blocks needed to build a <a href="https://github.com/emilemathieu/blog_cnn">deep learning library</a> in a few lines of Python. You'll learn how to build your (very) light-weight version of <a href="https://pytorch.org">PyTorch</a>!
  </p>

</p>
    
    
    

  </article>
</div>

        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://emilemathieu.fr/posts/2018/08/pubs/" rel="permalink">10 Best Beer Gardens In Oxford
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  7 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2018-08-07T00:00:00+01:00">August 07, 2018</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>If you are passing by Oxford as a tourist, a visiting student or have already been living there for a while, you cannot miss the enthralling excitement happening around the many bars and pubs scattered in the city. There are especially a significant selection of front and back gardens which offer the perfect place to drink a pint or a glass of Pimm's. I hope you'll love my hand-curated list of the most thrilling beer gardens one can find in <i> The City of Dreaming Spires</i>!</p>

</p>
    
    
    

  </article>
</div>

        
      </div>
    </div>
  
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<a href="/sitemap/">Sitemap</a>
<!-- end custom footer snippets -->

        

<div class="page__footer-follow">
  <ul class="social-icons">
    
    
    
    
      <li><a href="http://github.com/emilemathieu"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="https://emilemathieu.fr/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 Emile Mathieu. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    <script src="https://emilemathieu.fr/assets/js/main.min.js"></script>




  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90612705-1', 'auto');
  ga('send', 'pageview');
</script>






  </body>
  <script src="https://emilemathieu.fr/assets/js/mode-switcher.js"></script>
</html>

