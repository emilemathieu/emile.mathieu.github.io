--
---
References
==========

@article{oyanedel2021dynamic,
  title     = {A Dynamic Simulation Model to Support Reduction in Illegal Trade within Legal Wildlife Markets},
  author    = {Oyanedel, Rodrigo and Gelcich, Stefan and Mathieu, Emile and {Milner-Gulland}, E. J.},
  year      = {2021},
  month     = aug,
  journal   = {Conservation Biology},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn      = {0888-8892},
  doi       = {10.1111/cobi.13814},
  pdf       = {https://conbio.onlinelibrary.wiley.com/doi/epdf/10.1111/cobi.13814}
}

@inproceedings{mathieu2021Contrastive,
  abstract  = {Learning representations of stochastic processes is an emerging problem in machine learning with applications from meta-learning to physical object models to time series. Typical methods rely on exact reconstruction of observations, but this approach breaks down as observations become high-dimensional or noise distributions become complex. To address this, we propose a unifying framework for learning contrastive representations of stochastic processes (CReSP) that does away with exact reconstruction. We dissect potential use cases for stochastic process representations, and propose methods that accommodate each. Empirically, we show that our methods are effective for learning representations of periodic functions, 3D objects and dynamical processes. Our methods tolerate noisy high-dimensional observations better than traditional approaches, and the learned representations transfer to a range of downstream tasks.},
  title     = {On Contrastive Representations of Stochastic Processes},
  author    = {Mathieu, Emile and Foster, Adam and Teh, Yee Whye},
  booktitle = {Advances in Neural Information Processing Systems 33},
  year      = {2021},
  publisher = {Curran Associates, Inc.},
  arxiv     = {http://arxiv.org/abs/2106.10052},
  slides    = {https://drive.google.com/file/d/1VNXFwFXbOyEabKGOlbWYhczh4xSitzvh/view?usp=share_link},
  software  = {https://github.com/ae-foster/cresp}
}

@inproceedings{mathieu2019Riemannian,
  abstract  = {Normalizing flows have shown great promise for modelling flexible probability distributions in a computationally tractable way. However, whilst data is often naturally described on Riemannian manifolds such as spheres, torii, and hyperbolic spaces, most normalizing flows implicitly assume a flat geometry, making them either misspecified or ill-suited in these situations. To overcome this problem, we introduce Riemannian continuous normalizing flows, a model which admits the parametrization of flexible probability measures on smooth manifolds by defining flows as the solution to ordinary differential equations. We show that this approach can lead to substantial improvements on both synthetic and real-world data when compared to standard flows or previously introduced projected flows.},
  title     = {Riemannian Continuous Normalizing Flows},
  author    = {Mathieu, Emile and Nickel, Maximilian},
  booktitle = {Advances in Neural Information Processing Systems 33},
  year      = {2020},
  pages     = {2503--2515},
  publisher = {Curran Associates, Inc.},
  pdf       = {https://papers.nips.cc/paper/2020/file/1aa3d9c6ce672447e1e5d0f1b5207e85-Paper.pdf},
  arxiv     = {https://arxiv.org/abs/2006.10605},
  slides    = {https://drive.google.com/file/d/1ZhQAg1TDGkQxa3zf20ciCRsl3CQDMXV_/view?usp=share_link},
  poster    = {https://drive.google.com/file/d/1G9hMEAB5QNrYtWfosLuHszLtveGi7dQd/view?usp=share_link}
}

@inproceedings{mathieu2019Continuous,
  abstract  = {The Variational Auto-Encoder (VAE) is a popular method for learning a generative model and embeddings of the data. Many real datasets are hierarchically structured. However, traditional VAEs map data in a Euclidean latent space which cannot efficiently embed tree-like structures. Hyperbolic spaces with negative curvature can. We therefore endow VAEs with a Poincaré ball model of hyperbolic geometry as a latent space and rigorously derive the necessary methods to work with two main Gaussian generalisations on that space. We empirically show better generalisation to unseen data than the Euclidean counterpart, and can qualitatively and quantitatively better recover hierarchical structures.},
  title     = {Continuous Hierarchical Representations with Poincar\'{e} Variational Auto-Encoders},
  author    = {Mathieu, Emile and Le Lan, Charline and Maddison, Chris J. and Tomioka, Ryota and Teh, Yee Whye},
  booktitle = {Advances in Neural Information Processing Systems 32},
  pages     = {12565--12576},
  year      = {2019},
  publisher = {Curran Associates, Inc.},
  url       = {http://papers.neurips.cc/paper/9420-continuous-hierarchical-representations-with-poincare-variational-auto-encoders},
  pdf       = {http://papers.nips.cc/paper/9420-continuous-hierarchical-representations-with-poincare-variational-auto-encoders.pdf},
  arxiv     = {https://arxiv.org/abs/1901.06033},
  software  = {https://github.com/emilemathieu/pvae},
  slides    = {https://drive.google.com/file/d/1UFzGPkah_TfHPaFLBoIOR23-33S2spwW/view?usp=share_link},
  poster    = {https://drive.google.com/file/d/1eHTJnqgct05l5Mis9idG8fCZtADsn9jw/view?usp=share_link}
}

@inproceedings{mathieu2019Disentaling,
  abstract  = {We develop a generalisation of disentanglement in VAEs---decomposition of the latent representation---characterising it as the fulfilment of two factors: a) the latent encodings of the data having an appropriate level of overlap, and b) the aggregate encoding of the data conforming to a desired structure, represented through the prior. Decomposition permits disentanglement, i.e. explicit independence between latents, as a special case, but also allows for a much richer class of properties to be imposed on the learnt representation, such as sparsity, clustering, independent subspaces, or even intricate hierarchical dependency relationships. We show that the β-VAE varies from the standard VAE predominantly in its control of latent overlap and that for the standard choice of an isotropic Gaussian prior, its objective is invariant to rotations of the latent representation. Viewed from the decomposition perspective, breaking this invariance with simple manipulations of the prior can yield better disentanglement with little or no detriment to reconstructions. We further demonstrate how other choices of prior can assist in producing different decompositions and introduce an alternative training objective that allows the control of both decomposition factors in a principled manner.},
  title     = {Disentangling Disentanglement in Variational Autoencoders},
  author    = {Mathieu, Emile and Rainforth, Tom and Siddharth, N and Teh, Yee Whye},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  pages     = {4402--4412},
  year      = {2019},
  volume    = 97,
  series    = {Proceedings of Machine Learning Research},
  address   = {Long Beach, California, USA},
  month     = {09--15 Jun},
  publisher = {PMLR},
  project   = {bigbayes},
  pdf       = {http://proceedings.mlr.press/v97/mathieu19a/mathieu19a.pdf},
  url       = {http://proceedings.mlr.press/v97/mathieu19a.html},
  arxiv     = {https://arxiv.org/abs/1812.02833},
  software  = {https://github.com/iffsid/disentangling-disentanglement},
  slides    = {https://drive.google.com/file/d/1E4ywZ62Hhr4txVo9vSd5fo-5w6ZUEm2O/view?usp=share_link},
  poster    = {https://drive.google.com/file/d/1aE7_euQF8n4ja5vuJr2AfEYtrqvPZheB/view?usp=share_link},
  oral      = {https://icml.cc/media/Slides/icml/2019/halla(12-11-00)-12-11-35-4811-disentangling_d.pdf}
}
  
  @inproceedings{BloemReddy2018Sampling,
  abstract  = {Empirical evidence suggests that heavy-tailed degree distributions occurring in many real networks are well-approximated by power laws with exponents η that may take values either less than and greater than two. Models based on various forms of exchangeability are able to capture power laws with $\eta<2$, and admit tractable inference algorithms; we draw on previous results to show that $\eta>2$ cannot be generated by the forms of exchangeability used in existing random graph models. Preferential attachment models generate power law exponents greater than two, but have been of limited use as statistical models due to the inherent difficulty of performing inference in non-exchangeable models. Motivated by this gap, we design and implement inference algorithms for a recently proposed class of models that generates $\eta$ of all possible values. We show that although they are not exchangeable, these models have probabilistic structure amenable to inference. Our methods make a large class of previously intractable models useful for statistical inference.},
  author    = {Bloem-Reddy, Benjamin and Foster, Adam and Mathieu, Emile and Teh, Yee Whye},
  booktitle = {Conference on Uncertainty in Artificial Intelligence},
  pdf       = {http://auai.org/uai2018/proceedings/papers/185.pdf},
  title     = {Sampling and Inference for Beta Neutral-to-the-Left Models of Sparse Networks},
  arxiv     = {https://arxiv.org/abs/1807.03113},
  pdf       = {http://auai.org/uai2018/proceedings/papers/185.pdf},
  slides    = {https://github.com/ae-foster/bntl_presentation/blob/master/main.pdf},
  software  = {https://github.com/emilemathieu/NTL.jl},
  project   = {bigbayes},
  month     = {August},
  year      = {2018},
  oral      = {https://www.youtube.com/watch?v=0PlIFXBpIgU}
}
  
  @inproceedings{bloemreddy2017Sampling,
  abstract = {We consider the problem of sampling a sequence from a discrete random probability measure (RPM) with countable support, under (probabilistic) constraints of finite memory and computation. A canonical example is sampling from the Dirichlet Process, which can be accomplished using its stick-breaking representation and lazy initialization of its atoms. We show that efficiently lazy initialization is possible if and only if a size-biased representation of the discrete RPM is used. For models constructed from such discrete RPMs, we consider the implications for generic particle-based inference methods in probabilistic programming systems. To demonstrate, we implement SMC for Normalized Inverse Gaussian Process mixture models in Turing.},
  title    = {Sampling and inference for discrete random probability measures in probabilistic programs},
  author   = {Bloem-Reddy, Benjamin and Mathieu, Emile and Foster, Adam and Rainforth, Tom and Ge, Hong and Lomelí, María and Ghahramani, Zoubin and Teh, Yee Whye},
  journal  = {NIPS Workshop on Advances in Approximate Bayesian Inference},
  url      = {http://approximateinference.org/2017/accepted/Bloem-ReddyEtAl2017.pdf},
  project  = {bigbayes},
  year     = {2017}
}
  
  @inproceedings{miao2022incorporating,
  title     = {On Incorporating Inductive Biases into {{VAEs}}},
  booktitle = {Tenth International Conference on Learning Representations},
  author    = {Miao, Ning and Mathieu, Emile and N, Siddharth and Teh, Yee Whye and Rainforth, Tom},
  year      = {2022},
  arxiv     = {https://arxiv.org/abs/2106.13746},
  url       = {https://openreview.net/pdf?id=nzvbBD_3J-g},
  software  = {https://github.com/NingMiao/InstaAug}
}
  
  @inproceedings{debortoli2022Riemannian,
  title       = {Riemannian Score-Based Generative Modelling},
  author      = {Valentin De Bortoli and Emile Mathieu and Michael John Hutchinson and James Thornton and Yee Whye Teh and Arnaud Doucet},
  booktitle   = {Advances in Neural Information Processing Systems},
  editor      = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  year        = {2022},
  url         = {https://openreview.net/forum?id=oDRQGo8I7P},
  arxiv       = {https://arxiv.org/abs/2202.02763},
  software    = {https://github.com/oxcsml/riemannian-score-sde},
  poster      = {https://nips.cc/media/PosterPDFs/NeurIPS%202022/54590.png?t=1669208509.0735784},
  outstanding = {https://blog.neurips.cc/2022/11/21/announcing-the-neurips-2022-awards/}
}
  @inproceedings{thornton2022riemannian,
  title     = {Riemannian Diffusion Schr\"odinger Bridge},
  booktitle = {ICML Workshop on Continuous Time Methods for Machine Learning},
  author    = {Thornton, James and Hutchinson, Michael and Mathieu, Emile and De Bortoli, Valentin and Teh, Yee Whye and Doucet, Arnaud},
  year      = {2022}
}
  
  @inproceedings{yim2023SE,
  title     = {{{SE}}(3) Diffusion Model with Application to Protein Backbone Generation},
  author    = {Yim, Jason and Trippe, Brian L. and De Bortoli, Valentin and Mathieu, Emile and Doucet, Arnaud and Barzilay, Regina and Jaakkola, Tommi},
  year      = {2023},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  url       = {https://openreview.net/forum?id=m8OUBymxwv},
  arxiv     = {http://arxiv.org/abs/2302.02277},
  software  = {https://github.com/jasonkyuyim/se3_diffusion},
  langid    = {english}
}
  @inproceedings{miao2023Learning,
  title     = {Learning {{Instance-Specific Augmentations}} by {{Capturing Local Invariances}}},
  author    = {Miao, Ning and Rainforth, Tom and Mathieu, Emile and Dubois, Yann and Teh, Yee Whye and Foster, Adam and Kim, Hyunjik},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  year      = {2023},
  url       = {https://openreview.net/forum?id=7W1uE3BjPO&noteId=s62NlRKrXm},
  arxiv     = {https://arxiv.org/abs/2206.00051},
  langid    = {english}
}
  
  @misc{phillips2022Spectral,
  title        = {Spectral {{Diffusion Processes}}},
  author       = {Phillips, Angus and Seror, Thomas and Hutchinson, Michael and De Bortoli, Valentin and Doucet, Arnaud and Mathieu, Emile},
  year         = {2022},
  month        = nov,
  number       = {arXiv:2209.14125},
  primaryclass = {cs, stat},
  publisher    = {{arXiv}},
  url          = {http://arxiv.org/abs/2209.14125}
}
  

@article{fishman2023Diffusion,
  title   = {Diffusion Models for Constrained Domains},
  author  = {Fishman, Nic and Klarner, Leo and De Bortoli, Valentin and Mathieu, Emile and Hutchinson, Michael},
  journal = {Transactions on Machine Learning Research},
  year    = {2023},
  url     = {https://openreview.net/forum?id=xuWTFQ4VGO},
  note    = {Under review}
}


@inproceedings{fishman2023Metropolis,
  title     = {Metropolis Sampling for Constrained Diffusion Models},
  author    = {Fishman, Nic and Klarner, Leo and Mathieu, Emile and Hutchinson, Michael and De Bortoli, Valentin},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pages     = {62296--62331},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2023/file/c47bfcc8e2eccdc540fad1e25f13aa4d-Paper-Conference.pdf},
  volume    = {36},
  year      = {2023}
}


@inproceedings{mathieu2023Geometric,
  author    = {Mathieu, Emile and Dutordoir, Vincent and Hutchinson, Michael and De Bortoli, Valentin and Teh, Yee Whye and Turner, Richard},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pages     = {53475--53507},
  publisher = {Curran Associates, Inc.},
  title     = {Geometric Neural Diffusion Processes},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2023/file/a797c2d2e0c1fdabf4d1ab8cd0b465c6-Paper-Conference.pdf},
  volume    = {36},
  year      = {2023}
}


@article{watson2023novo,
  title     = {De Novo Design of Protein Structure and Function with {{RFdiffusion}}},
  author    = {Watson, Joseph L. and Juergens, David and Bennett, Nathaniel R. and Trippe, Brian L. and Yim, Jason and Eisenach, Helen E. and Ahern, Woody and Borst, Andrew J. and Ragotte, Robert J. and Milles, Lukas F. and Wicky, Basile I. M. and Hanikel, Nikita and Pellock, Samuel J. and Courbet, Alexis and Sheffler, William and Wang, Jue and Venkatesh, Preetham and Sappington, Isaac and Torres, Susana V{\'a}zquez and Lauko, Anna and De Bortoli, Valentin and Mathieu, Emile and Ovchinnikov, Sergey and Barzilay, Regina and Jaakkola, Tommi S. and DiMaio, Frank and Baek, Minkyung and Baker, David},
  year      = {2023},
  month     = jul,
  journal   = {Nature},
  pages     = {1--3},
  publisher = {{Nature Publishing Group}},
  issn      = {1476-4687},
  doi       = {10.1038/s41586-023-06415-8},
  copyright = {2023 The Author(s), under exclusive licence to Springer Nature Limited},
  langid    = {english}
}

@inproceedings{Midgley2023SE3,
  author    = {Midgley, Laurence and Stimper, Vincent and Antor\'{a}n, Javier and Mathieu, Emile and Sch\"{o}lkopf, Bernhard and Hern\'{a}ndez-Lobato, Jos\'{e} Miguel},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pages     = {79200--79225},
  publisher = {Curran Associates, Inc.},
  title     = {SE(3) Equivariant Augmented Coupling Flows},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2023/file/fa55eb802a531c8087e225ecf2dcfbca-Paper-Conference.pdf},
  volume    = {36},
  year      = {2023}
}

@misc{didi2024frameworkconditionaldiffusionmodelling,
  title         = {A framework for conditional diffusion modelling with applications in motif scaffolding for protein design},
  author        = {Kieran Didi and Francisco Vargas and Simon V Mathis and Vincent Dutordoir and Emile Mathieu and Urszula J Komorowska and Pietro Lio},
  year          = {2024},
  eprint        = {2312.09236},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2312.09236}
}


@article{yim2024improved,
  title   = {Improved motif-scaffolding with {SE}(3) flow matching},
  author  = {Jason Yim and Andrew Campbell and Emile Mathieu and Andrew Y. K. Foong and Michael Gastegger and Jose Jimenez-Luna and Sarah Lewis and Victor Garcia Satorras and Bastiaan S. Veeling and Frank Noe and Regina Barzilay and Tommi Jaakkola},
  journal = {Transactions on Machine Learning Research},
  issn    = {2835-8856},
  year    = {2024},
  url     = {https://openreview.net/forum?id=fa1ne8xDGn},
  note    = {}
}

@misc{denker2024deftefficientfinetuningdiffusion,
  title         = {DEFT: Efficient Fine-Tuning of Diffusion Models by Learning the Generalised $h$-transform},
  author        = {Alexander Denker and Francisco Vargas and Shreyas Padhy and Kieran Didi and Simon Mathis and Vincent Dutordoir and Riccardo Barbano and Emile Mathieu and Urszula Julia Komorowska and Pietro Lio},
  year          = {2024},
  eprint        = {2406.01781},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2406.01781}
}

@inproceedings{shysheya2024on,
  title     = {On conditional diffusion models for {PDE} simulations},
  author    = {Aliaksandra Shysheya and Cristiana Diaconu and Federico Bergamin and Paris Perdikaris and Jos{\'e} Miguel Hern{\'a}ndez-Lobato and Richard E. Turner and Emile Mathieu},
  booktitle = {The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year      = {2024},
  url       = {https://openreview.net/forum?id=nQl8EjyMzh}
}